{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib\n",
    "pip install PyQt5\n",
    "pip install numpy\n",
    "pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-67757f6be005>:5: DeprecationWarning: Please use `gaussian_filter1d` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import gaussian_filter1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, os, copy\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "plt.ion\n",
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_validity(current_data_value, previous_average):\n",
    "def check_diff_invalidity(current_data_value, previous_average):    \n",
    "    epsilon = 0.75\n",
    "    return not(np.linalg.norm(current_data_value - previous_average) <= epsilon)\n",
    "\n",
    "def check_stat_invalidity(current_data_value, previous_average):\n",
    "    epsilon = 1.0\n",
    "    statistical_mean = np.array([3.45, -2.4, 0.75])\n",
    "    return np.linalg.norm(current_data_value - statistical_mean) <= epsilon\n",
    "\n",
    "def check_stat_and_diff_invalidity(current_data_value, previous_average, t, latest_valid_index):\n",
    "\n",
    "    if latest_valid_index==-1:\n",
    "        previous_average = current_data_value\n",
    "            \n",
    "    invalid = check_diff_invalidity(current_data_value, previous_average) or check_stat_invalidity(current_data_value, previous_average)\n",
    "\n",
    "    return invalid\n",
    "    \n",
    "    # return check_diff_invalidity(current_data_value, previous_average) and check_stat_invalidity(current_data_value, previous_average)\n",
    "    # return check_diff_invalidity(current_data_value, previous_average) or check_stat_invalidity(current_data_value, previous_average)\n",
    "\n",
    "def filter_outliers(data):\n",
    "\n",
    "    filter_size = 3\n",
    "    # epsilon = 0.5 * np.ones(3)\n",
    "    epsilon = 0.5\n",
    "    alt_data = copy.deepcopy(data)\n",
    "    averages = np.zeros_like(data)\n",
    "    validity = np.ones(data.shape[0])\n",
    "    latest_valid_index = -1\n",
    "\n",
    "    # Initialize average.\n",
    "    averages[0] = data[0]    \n",
    "\n",
    "    print(\"Hello, about to start filtering.\")    \n",
    "\n",
    "\t# Statistical mode check\n",
    "    if check_stat_invalidity(alt_data[0], averages[0]):\n",
    "        validity[0] = 0\n",
    "\n",
    "    for k in range(1, data.shape[0]):\n",
    "        \n",
    "        # Check validity\n",
    "        # If we think it's an outlying datapoint based on averages, rewrite the data to last valid data point, \n",
    "        # and set validity to 0. \n",
    "        # if not(check_diff_validity(alt_data[k], averages[k-1])):\n",
    "        # if not(check_stat_validity(alt_data[k], averages[k-1])):   \n",
    "        if check_stat_and_diff_invalidity(alt_data[k], averages[k-1], k, latest_valid_index):\n",
    "            alt_data[k] = copy.deepcopy(alt_data[k-1])\n",
    "            validity[k] = 0.\n",
    "            # The average is now defined as the mean over the previous filter_size - 1 elements, and the current element.. \n",
    "            # averages[k] = alt_data[max(0, k+1-filter_size):k+1].mean(axis=0)\n",
    "            averages[k] = alt_data[max(latest_valid_index, k+1-filter_size):k+1].mean(axis=0)\n",
    "        else:            \n",
    "            # Special case ofr us encountering the first valid data point afer a stream of invalid data points. \n",
    "            if latest_valid_index == -1:\n",
    "                # Set average to current value.. \n",
    "                averages[k] = alt_data[k]\n",
    "            else:\n",
    "                # The average is now defined as the mean over the previous filter_size - 1 elements, and the current element.. \n",
    "                # averages[k] = alt_data[max(0, k+1-filter_size):k+1].mean(axis=0)\n",
    "                averages[k] = alt_data[max(latest_valid_index, k+1-filter_size):k+1].mean(axis=0)\n",
    "\n",
    "            # Set the last valid index to current timepoint. \n",
    "            latest_valid_index = k\n",
    "\n",
    "        # print(\"#######################\")\n",
    "        # print(\"Filtering timestep: \", k)\n",
    "        # print(data[k], averages[k-1], np.linalg.norm(data[k] - averages[k-1]))\n",
    "\n",
    "        # The average is now defined as the mean over the previous filter_size - 1 elements, and the current element.. \n",
    "        # averages[k] = alt_data[max(0, k+1-filter_size):k+1].mean(axis=0)\n",
    "    \n",
    "    return averages, validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_position(valid=None, position_sequence=None):\n",
    "\t\n",
    "\tfrom scipy import interpolate\n",
    "\t\n",
    "\t# Interp1d from Scipy expects the last dimension to be the dimension we are ninterpolating over. \t\n",
    "\tvalid_positions = np.swapaxes(position_sequence[valid==1], 1, 0)\n",
    "\tvalid_times = np.where(valid==1)[0]\n",
    "\tquery_times = np.arange(0, len(position_sequence))\n",
    "\n",
    "\t# Create interpolating function. \n",
    "\tinterpolating_function = interpolate.interp1d(valid_times, valid_positions)\n",
    "\n",
    "\t# Query interpolating function. \n",
    "\tinterpolated_positions = interpolating_function(query_times)\n",
    "\n",
    "\t# Swap axes back and return. \n",
    "\treturn np.swapaxes(interpolated_positions, 1, 0)\n",
    "\n",
    "def interpolate_orientation(valid=None, orientation_sequence=None):\n",
    "\n",
    "\tfrom scipy.spatial.transform import Rotation as R\n",
    "\tfrom scipy.spatial.transform import Slerp\n",
    "\n",
    "\tvalid_orientations = orientation_sequence[valid==1]\n",
    "\trotation_sequence = R.concatenate(R.from_quat(valid_orientations))\n",
    "\tvalid_times = np.where(valid==1)[0]\n",
    "\tquery_times = np.arange(0, len(orientation_sequence))\n",
    "\n",
    "\t# Create slerp object. \n",
    "\tslerp_object = Slerp(valid_times, rotation_sequence)\n",
    "\n",
    "\t# Query the slerp object. \n",
    "\tinterpolated_rotations = slerp_object(query_times)\n",
    "\n",
    "\t# Convert to quaternions.\n",
    "\tinterpolated_quaternion_sequence = interpolated_rotations.as_quat()\n",
    "\t\n",
    "\treturn interpolated_quaternion_sequence\n",
    "\n",
    "def interpolate_valid_data(valid_data, validity):\n",
    "\n",
    "    # Really this should follow the exact same pipeline as remaining dataloader stuff.. \n",
    "\t# Last elements will always be valid. \n",
    "\t# First may not be - backfill this. \n",
    "\n",
    "\t# Phase 2.a : The first few elements may not be valid (here, the last elements always will be). \n",
    "\t# If so, backfill first few elements with the first valid element. \n",
    "\tfirst_valid_index = np.where(validity)[0][0]\n",
    "\tif first_valid_index>0:\n",
    "\t\tvalid_data[:first_valid_index] = valid_data[first_valid_index]\n",
    "\t\tvalidity[:first_valid_index] = 1\n",
    "\n",
    "\t# Phase 2.b : Now that we have valid starts and ends, interpolate the data. \n",
    "\tinterpolated_data = interpolate_position(valid=validity, position_sequence=valid_data)\n",
    "\t\n",
    "\treturn interpolated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def old_process_data(data):\n",
    "\n",
    "#     # Select Posiiton Data.\n",
    "#     processed_data = data[1:, 10:13]\n",
    "#     filtered_data, validity = filter_outliers(processed_data)\n",
    "#     return filtered_data\n",
    "\n",
    "def process_data(data):\n",
    "\n",
    "    # Select Posiiton Data.\n",
    "    processed_data = data[1:, 10:13]\n",
    "    filtered_data, validity = filter_outliers(processed_data)\n",
    "    # filtered_data = processed_data\n",
    "\n",
    "    # Interpolate data - second phase \n",
    "    interpolated_data = interpolate_valid_data(filtered_data, validity)\n",
    "\n",
    "    # Smoothen data\n",
    "    kernel_bandwidth = 3\n",
    "    smoothed_data = gaussian_filter1d(interpolated_data,kernel_bandwidth,axis=0,mode='nearest')\n",
    "\n",
    "    # return filtered_data\n",
    "    # return interpolated_data\n",
    "    return smoothed_data\n",
    "\n",
    "def plot_traj(original_data, processed_data=None):\n",
    "    # plt.close()\n",
    "    plt.figure()\n",
    "    plt.plot(original_data)\n",
    "    if processed_data is not None:\n",
    "        plt.plot(processed_data, 'o', markersize=2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tshankar\\Desktop\\Research\\Data\\NDAX\\dataset_prelim_2\\dataset_prelim_2\n"
     ]
    }
   ],
   "source": [
    "cd C:\\Users\\tshankar\\Desktop\\Research\\Data\\NDAX\\dataset_prelim_2\\dataset_prelim_2\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = sorted(glob.glob(\"*.csv\"))\n",
    "\n",
    "for k,v in enumerate(filelist):\n",
    "\n",
    "    # if not(v.startswith(\"stock_cupboard\")):\n",
    "    if True:\n",
    "    \n",
    "        print(\"################\")\n",
    "        print(\"Processing file: \", k, \" , named: \", v)\n",
    "\n",
    "        # Extract from file\n",
    "        data = np.genfromtxt(v, delimiter=',')\n",
    "        # process\n",
    "        pdata = process_data(data)\n",
    "        # plot\n",
    "        plot_traj(data[1:, 10:13], pdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('stock_cupboard_1.csv', delimiter=',')\n",
    "processed_data = data[1:, 10:13]\n",
    "filtered_data, validity = filter_outliers(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolated_data = interpolate_valid_data(filtered_data, validity)\n",
    "position_sequence = filtered_data\n",
    "valid = validity\n",
    "\n",
    "from scipy import interpolate\n",
    "\n",
    "# Interp1d from Scipy expects the last dimension to be the dimension we are ninterpolating over. \n",
    "valid_positions = np.swapaxes(position_sequence[valid==1], 1, 0)\n",
    "valid_times = np.where(valid==1)[0]\n",
    "query_times = np.arange(0, len(position_sequence))\n",
    "\n",
    "interpolating_function = interpolate.interp1d(valid_times, valid_positions)\n",
    "interpolated_positions = interpolating_function(query_times)\n",
    "interpolated_data = np.swapaxes(interpolated_positions, 1, 0)\n",
    "interpolated_data.shape\n",
    "\n",
    "plot_traj(data[1:, 10:13], interpolated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = sorted(glob.glob(\"*.csv\"))\n",
    "\n",
    "for k,v in enumerate(filelist):\n",
    "\n",
    "    # if not(v.startswith(\"stock_cupboard\")):\n",
    "    if True:\n",
    "    \n",
    "        print(\"################\")\n",
    "        print(\"Processing file: \", k, \" , named: \", v)\n",
    "\n",
    "        # Extract from file\n",
    "        data = np.genfromtxt(v, delimiter=',')\n",
    "        # process\n",
    "        pdata = process_data(data)\n",
    "        # plot\n",
    "        plot_traj(data[1:, 10:13], pdata)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\"New_Task_Demo_Array_Filtered.npy\", allow_pickle=True)\n",
    "x[0]['demo'].shape\n",
    "\n",
    "for k in range(len(x)):\n",
    "    print(\"################\")\n",
    "    print(\"Processing file: \", k)\n",
    "\n",
    "    plot_traj(x[k]['demo'][:,6:9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
